# æ¨¡å‹

[TOC]

## deepseek-r1

DeepSeek's first-generation of reasoning models with comparable performance to  OpenAI-o1, including six dense models distilled from DeepSeek-R1 based  on Llama and Qwen.
DeepSeek çš„ç¬¬ä¸€ä»£æ¨ç†æ¨¡å‹ï¼Œæ€§èƒ½å¯ä¸ OpenAI-o1 ç›¸åª²ç¾ï¼ŒåŒ…æ‹¬ä» DeepSeek-R1 ä¸­æç‚¼å‡ºæ¥çš„åŸºäº Llama å’Œ Qwen çš„ 6 ä¸ªå¯†é›†æ¨¡å‹ã€‚

1.5b	7b	8b	14b	32b	70b	671b

##             llama3.3

New state of the art 70B model. Llama 3.3 70B offers similar performance compared to the Llama 3.1 405B model.
æœ€å…ˆè¿›çš„ 70B å‹å·ã€‚ä¸ Llama 3.3 70B å‹å·ç›¸æ¯”ï¼ŒLlama 3.1 405B æä¾›ç›¸ä¼¼çš„æ€§èƒ½ã€‚

tools	70b

##             phi4

Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.
Phi-4 æ˜¯ Microsoft çš„ 14B å‚æ•°ã€æœ€å…ˆè¿›çš„å¼€æ”¾æ¨¡å‹ã€‚

â€‹                                                              14b

##             llama3.2

Meta's Llama 3.2 goes small with 1B and 3B models. 
Meta çš„ Llama 3.2 é€šè¿‡ 1B å’Œ 3B æ¨¡å‹å˜å°ã€‚

â€‹                                                  tools å·¥å…·                                      1b                          3b

##             llama3.1

Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.
Llama 3.1 æ˜¯ Meta æ¨å‡ºçš„æœ€å…ˆè¿›çš„æ–°å‹å·ï¼Œæä¾› 8Bã€70B å’Œ 405B å‚æ•°å¤§å°ã€‚

â€‹                                                  tools å·¥å…·                                      8b                          70b                          405b

##             nomic-embed-text

A high-performing open embedding model with a large token context window.
å…·æœ‰å¤§å‹ token ä¸Šä¸‹æ–‡çª—å£çš„é«˜æ€§èƒ½å¼€æ”¾åµŒå…¥æ¨¡å‹ã€‚

â€‹                                      embedding

##             mistral

The 7B model released by Mistral AI, updated to version 0.3.
Mistral AI å‘å¸ƒçš„ 7B æ¨¡å‹ï¼Œæ›´æ–°åˆ° 0.3 ç‰ˆæœ¬ã€‚

â€‹                                                  tools å·¥å…·                                      7b

##             llama3

Meta Llama 3: The most capable openly available LLM to date
Meta Llama 3ï¼šè¿„ä»Šä¸ºæ­¢æœ€æœ‰èƒ½åŠ›çš„å…¬å¼€å¯ç”¨LLM

â€‹                                                              8b                          70b

##             qwen2.5

Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset,  encompassing up to 18 trillion tokens. The model supports up to 128K  tokens and has multilingual support. 
Qwen2.5 æ¨¡å‹åœ¨é˜¿é‡Œå·´å·´æœ€æ–°çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼ŒåŒ…å«å¤šè¾¾ 18 ä¸‡äº¿ä¸ªä»£å¸ã€‚è¯¥æ¨¡å‹æœ€å¤šæ”¯æŒ 128K ä¸ªä»¤ç‰Œï¼Œå¹¶å…·æœ‰å¤šè¯­è¨€æ”¯æŒã€‚

â€‹                                                  tools å·¥å…·                                      0.5b 0.5 å­—èŠ‚                          1.5b                          3b                          7b                          14b                          32b                          72b

##             qwen

Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters
Qwen 1.5 æ˜¯é˜¿é‡Œäº‘æ¨å‡ºçš„ä¸€ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‚æ•°èŒƒå›´ä» 0.5B åˆ° 110B ä¸ç­‰

â€‹                                                              0.5b 0.5 å­—èŠ‚                          1.8b                          4b                          7b                          14b                          32b                          72b                          110b

##             gemma

Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1
Gemma æ˜¯ç”± Google DeepMind æ„å»ºçš„ä¸€ç³»åˆ—è½»é‡çº§ã€æœ€å…ˆè¿›çš„å¼€æ”¾æ¨¡å‹ã€‚æ›´æ–°è‡³ç‰ˆæœ¬ 1.1

â€‹                                                              2b                          7b

##             qwen2

Qwen2 is a new series of large language models from Alibaba group
Qwen2 æ˜¯é˜¿é‡Œå·´å·´é›†å›¢æ¨å‡ºçš„ä¸€ç³»åˆ—æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹

â€‹                                                  tools å·¥å…·                                      0.5b 0.5 å­—èŠ‚                          1.5b                          7b                          72b

##             qwen2.5-coder

The latest series of Code-Specific Qwen models, with significant  improvements in code generation, code reasoning, and code fixing.
æœ€æ–°ç³»åˆ—çš„ç‰¹å®šäºä»£ç çš„ Qwen æ¨¡å‹ï¼Œåœ¨ä»£ç ç”Ÿæˆã€ä»£ç æ¨ç†å’Œä»£ç ä¿®å¤æ–¹é¢æœ‰é‡å¤§æ”¹è¿›ã€‚

â€‹                                                  tools å·¥å…·                                      0.5b 0.5 å­—èŠ‚                          1.5b                          3b                          7b                          14b                          32b

##             llava

ğŸŒ‹ LLaVA is a novel end-to-end trained large multimodal model that  combines a vision encoder and Vicuna for general-purpose visual and  language understanding. Updated to version 1.6.
ğŸŒ‹ LLaVA æ˜¯ä¸€ç§æ–°å‹çš„ç«¯åˆ°ç«¯è®­ç»ƒå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œå®ƒç»“åˆäº†è§†è§‰ç¼–ç å™¨å’Œ Vicunaï¼Œç”¨äºé€šç”¨è§†è§‰å’Œè¯­è¨€ç†è§£ã€‚æ›´æ–°è‡³ 1.6 ç‰ˆæœ¬ã€‚

â€‹                        vision è§†è§‰                                                              7b                          13b                          34b

##             gemma2

Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.
Google Gemma 2 æ˜¯ä¸€æ¬¾é«˜æ€§èƒ½ã€é«˜æ•ˆçš„å‹å·ï¼Œæœ‰ä¸‰ç§å°ºå¯¸å¯ä¾›é€‰æ‹©ï¼š2Bã€9B å’Œ 27Bã€‚

â€‹                                                              2b                          9b                          27b

##             llama2

Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.
Llama 2 æ˜¯åŸºç¡€è¯­è¨€æ¨¡å‹çš„é›†åˆï¼Œå‚æ•°èŒƒå›´ä» 7B åˆ° 70B ä¸ç­‰ã€‚

â€‹                                                              7b                          13b                          70b

##             phi3

Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.
Phi-3 æ˜¯ Microsoft æ¨å‡ºçš„è½»é‡çº§ 3B ï¼ˆMiniï¼‰ å’Œ 14B ï¼ˆMediumï¼‰ å…ˆè¿›å¼€æ”¾å¼å‹å·ç³»åˆ—ã€‚

â€‹                                                              3.8b                          14b

##             codellama

A large language model that can use text prompts to generate and discuss code.
ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨æ–‡æœ¬æç¤ºæ¥ç”Ÿæˆå’Œè®¨è®ºä»£ç ã€‚

â€‹                                                              7b                          13b                          34b                          70b

##             mxbai-embed-large

State-of-the-art large embedding model from mixedbread.ai
æ¥è‡ª mixedbread.ai çš„æœ€å…ˆè¿›çš„å¤§å‹åµŒå…¥æ¨¡å‹

â€‹                                      embedding åµŒå…¥

##             llama3.2-vision

Llama 3.2 Vision is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes.
Llama 3.2 Vision æ˜¯ 11B å’Œ 90B å¤§å°çš„æŒ‡ä»¤è°ƒæ•´å›¾åƒæ¨ç†ç”Ÿæˆæ¨¡å‹çš„é›†åˆã€‚

â€‹                        vision è§†è§‰                                                              11b                          90b

##             tinyllama

The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.
TinyLlama é¡¹ç›®æ˜¯ä¸€é¡¹å…¬å¼€çš„åŠªåŠ›ï¼Œæ—¨åœ¨åœ¨ 3 ä¸‡äº¿ä¸ªä»£å¸ä¸Šè®­ç»ƒä¸€ä¸ªç´§å‡‘çš„ 1.1B Llama æ¨¡å‹ã€‚

â€‹                                                              1.1b

##             mistral-nemo

A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.
å…·æœ‰ 128k ä¸Šä¸‹æ–‡é•¿åº¦çš„å…ˆè¿› 12B æ¨¡å‹ï¼Œç”± Mistral AI ä¸ NVIDIA åˆä½œæ„å»ºã€‚

â€‹                                                  tools å·¥å…·                                      12b

##             starcoder2

StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes: 3B, 7B and 15B parameters. 
StarCoder2 æ˜¯ä¸‹ä¸€ä»£é€æ˜è®­ç»ƒçš„å¼€æ”¾ä»£ç LLMsï¼Œæœ‰ä¸‰ç§å¤§å°ï¼š3Bã€7B å’Œ 15B å‚æ•°ã€‚

â€‹                                                              3b                          7b                          15b

##             snowflake-arctic-embed

A suite of text embedding models by Snowflake, optimized for performance.
Snowflake æä¾›çš„ä¸€å¥—æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œé’ˆå¯¹æ€§èƒ½è¿›è¡Œäº†ä¼˜åŒ–ã€‚

â€‹                                      embedding åµŒå…¥                                                  22m 22 åˆ†é’Ÿ                          33m 33 åˆ†é’Ÿ                          110m 110 åˆ†é’Ÿ                          137m 137 åˆ†é’Ÿ                          335m

##             deepseek-coder-v2

An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.
ä¸€ç§å¼€æº Mixture-of-Experts ä»£ç è¯­è¨€æ¨¡å‹ï¼Œå¯åœ¨ç‰¹å®šäºä»£ç çš„ä»»åŠ¡ä¸­å®ç°ä¸ GPT4-Turbo ç›¸å½“çš„æ€§èƒ½ã€‚

â€‹                                                              16b                          236b

##             deepseek-v3

A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.
ä¸€ä¸ªå¼ºå¤§çš„ä¸“å®¶æ··åˆ ï¼ˆMoEï¼‰ è¯­è¨€æ¨¡å‹ï¼Œæ€»å…±æœ‰ 671B ä¸ªå‚æ•°ï¼Œæ¯ä¸ªæ ‡è®°æ¿€æ´»äº† 37Bã€‚

â€‹                                                              671b

##             llama2-uncensored

Uncensored Llama 2 model by George Sung and Jarrad Hope.
æœªç»å®¡æŸ¥çš„ Llama 2 æ¨¡å‹ï¼Œç”± George Sung å’Œ Jarrad Hope è®¾è®¡ã€‚

â€‹                                                              7b                          70b

##             deepseek-coder

DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.
DeepSeek Coder æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ç¼–ç æ¨¡å‹ï¼Œä½¿ç”¨ 2 ä¸‡äº¿ä¸ªä»£ç å’Œè‡ªç„¶è¯­è¨€æ ‡è®°è¿›è¡Œè®­ç»ƒã€‚

â€‹                                                              1.3b                          6.7b 6.7 å­—èŠ‚                          33b

##             mixtral

A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes.
ä¸€ç»„ä¸“å®¶æ··åˆ ï¼ˆMoEï¼‰ æ¨¡å‹ï¼Œç”± Mistral AI ä»¥ 8x7b å’Œ 8x22b å‚æ•°å¤§å°æä¾›å¼€æ”¾æƒé‡ã€‚

â€‹                                                  tools å·¥å…·                                      8x7b                          8x22b

##             dolphin-mixtral

Uncensored, 8x7b and 8x22b fine-tuned models based on the Mixtral mixture of  experts models that excels at coding tasks. Created by Eric Hartford.
æœªç»å®¡æŸ¥çš„ 8x7b å’Œ 8x22b å¾®è°ƒæ¨¡å‹ï¼ŒåŸºäº Mixtral æ··åˆä¸“å®¶æ¨¡å‹ï¼Œæ“…é•¿ç¼–ç ä»»åŠ¡ã€‚ç”± Eric Hartford åˆ›å»ºã€‚

â€‹                                                              8x7b                          8x22b

##             codegemma

CodeGemma is a collection of powerful, lightweight models that can perform a  variety of coding tasks like fill-in-the-middle code completion, code  generation, natural language understanding, mathematical reasoning, and  instruction following.
CodeGemma æ˜¯ä¸€ç»„åŠŸèƒ½å¼ºå¤§çš„è½»é‡çº§æ¨¡å‹ï¼Œå¯ä»¥æ‰§è¡Œå„ç§ç¼–ç ä»»åŠ¡ï¼Œå¦‚å¡«å……ä¸­é—´ä»£ç å®Œæˆã€ä»£ç ç”Ÿæˆã€è‡ªç„¶è¯­è¨€ç†è§£ã€æ•°å­¦æ¨ç†å’ŒæŒ‡ä»¤è·Ÿè¸ªã€‚

â€‹                                                              2b                          7b

##             openthinker

A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-R1.
ä¸€ä¸ªå®Œå…¨å¼€æºçš„æ¨ç†æ¨¡å‹ç³»åˆ—ï¼Œä½¿ç”¨é€šè¿‡è’¸é¦ DeepSeek-R1 å¾—å‡ºçš„æ•°æ®é›†æ„å»ºã€‚

â€‹                                                              7b                          32b

##             phi

Phi-2: a 2.7B language model by Microsoft Research that demonstrates  outstanding reasoning and language understanding capabilities.
Phi-2ï¼šMicrosoft Research çš„ 2.7B è¯­è¨€æ¨¡å‹ï¼Œå±•ç¤ºäº†å‡ºè‰²çš„æ¨ç†å’Œè¯­è¨€ç†è§£èƒ½åŠ›ã€‚

â€‹                                                              2.7b

##             bge-m3

BGE-M3 is a new model from BAAI distinguished for its versatility in  Multi-Functionality, Multi-Linguality, and Multi-Granularity.
BGE-M3 æ˜¯ BAAI çš„ä¸€æ¬¾æ–°å‹å·ï¼Œä»¥å…¶åœ¨å¤šåŠŸèƒ½ã€å¤šè¯­è¨€å’Œå¤šç²’åº¦æ–¹é¢çš„å¤šåŠŸèƒ½æ€§è€Œè‘—ç§°ã€‚

â€‹                                      embedding åµŒå…¥                                                  567m

##             minicpm-v

A series of multimodal LLMs (MLLMs) designed for vision-language understanding.
ä¸€ç³»åˆ—ä¸“ä¸ºè§†è§‰-è¯­è¨€ç†è§£è€Œè®¾è®¡çš„å¤šæ¨¡æ€ LLMs ï¼ˆMLLMï¼‰ã€‚

â€‹                        vision è§†è§‰                                                              8b

##             llava-llama3

A LLaVA model fine-tuned from Llama 3 Instruct with better scores in several benchmarks.
ä» Llama 3 Instruct å¾®è°ƒçš„ LLaVA æ¨¡å‹ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¾—åˆ†æ›´é«˜ã€‚

â€‹                        vision è§†è§‰                                                              8b

##             wizardlm2

State of the art large language model from Microsoft AI with improved  performance on complex chat, multilingual, reasoning and agent use  cases.
æ¥è‡ª Microsoft AI çš„æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨å¤æ‚èŠå¤©ã€å¤šè¯­è¨€ã€æ¨ç†å’Œä»£ç†ç”¨ä¾‹ä¸Šçš„æ€§èƒ½å¾—åˆ°äº†æ”¹è¿›ã€‚

â€‹                                                              7b                          8x22b

##             dolphin-mistral

The uncensored Dolphin model based on Mistral that excels at coding tasks. Updated to version 2.8.
åŸºäº Mistral çš„æœªç»å®¡æŸ¥çš„ Dolphin æ¨¡å‹ï¼Œæ“…é•¿ç¼–ç ä»»åŠ¡ã€‚æ›´æ–°è‡³ 2.8 ç‰ˆã€‚

â€‹                                                              7b

##             all-minilm

Embedding models on very large sentence level datasets.
å°†æ¨¡å‹åµŒå…¥åˆ°éå¸¸å¤§çš„å¥å­çº§æ•°æ®é›†ä¸Šã€‚

â€‹                                      embedding åµŒå…¥                                                  22m 22 åˆ†é’Ÿ                          33m

##             smollm2          

SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.
SmolLM2 æ˜¯ä¸€ç³»åˆ—ç´§å‡‘çš„è¯­è¨€æ¨¡å‹ï¼Œæœ‰ä¸‰ç§å°ºå¯¸å¯ä¾›é€‰æ‹©ï¼š135Mã€360M å’Œ 1.7B å‚æ•°ã€‚

â€‹                                                  tools å·¥å…·                                      135m 135 åˆ†é’Ÿ                          360m 360 ç±³                          1.7b 

##             dolphin-llama3

Dolphin 2.9 is a new model with 8B and 70B sizes by Eric Hartford based on  Llama 3 that has a variety of instruction, conversational, and coding  skills.
Dolphin 2.9 æ˜¯ Eric Hartford åŸºäº Llama 8 å¼€å‘çš„ 70B å’Œ 3B å¤§å°çš„æ–°æ¨¡å‹ï¼Œå…·æœ‰å¤šç§æ•™å­¦ã€å¯¹è¯å’Œç¼–ç æŠ€èƒ½ã€‚

â€‹                                                              8b                          70b

##             command-r

Command R is a Large Language Model optimized for conversational interaction and long context tasks.
Command R æ˜¯ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé’ˆå¯¹å¯¹è¯äº¤äº’å’Œé•¿ä¸Šä¸‹æ–‡ä»»åŠ¡è¿›è¡Œäº†ä¼˜åŒ–ã€‚

â€‹                                                  tools å·¥å…·                                      35b

##             orca-mini

A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.
ä» 30 äº¿ä¸ªå‚æ•°åˆ° 700 äº¿ä¸ªå‚æ•°ä¸ç­‰çš„é€šç”¨æ¨¡å‹ï¼Œé€‚ç”¨äºå…¥é—¨çº§ç¡¬ä»¶ã€‚

â€‹                                                              3b                          7b                          13b                          70b

##             yi

Yi 1.5 is a high-performing, bilingual language model.
Yi 1.5 æ˜¯ä¸€ç§é«˜æ€§èƒ½çš„åŒè¯­è¯­è¨€æ¨¡å‹ã€‚

â€‹                                                              6b                          9b                          34b

##             hermes3

Hermes 3 is the latest version of the flagship Hermes series of LLMs by Nous Research
çˆ±é©¬ä»• 3 æ˜¯ Nous Research LLMs çš„æ——èˆ°çˆ±é©¬ä»•ç³»åˆ—çš„æœ€æ–°ç‰ˆæœ¬

â€‹                                                  tools å·¥å…·                                      3b                          8b                          70b                          405b

##             phi3.5

A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger sized models.
ä¸€ä¸ªè½»é‡çº§çš„ AI æ¨¡å‹ï¼Œå…·æœ‰ 38 äº¿ä¸ªå‚æ•°ï¼Œå…¶æ€§èƒ½è¶…è¿‡äº†ç±»ä¼¼å’Œæ›´å¤§å°ºå¯¸çš„æ¨¡å‹ã€‚

â€‹                                                              3.8b

##             dolphin3

Dolphin 3.0 Llama 3.1 8B ğŸ¬ is the next generation of the Dolphin series of  instruct-tuned models designed to be the ultimate general purpose local  model, enabling coding, math, agentic, function calling, and general use cases.
Dolphin 3.0 Llama 3.1 8B ğŸ¬ æ˜¯ Dolphin ç³»åˆ—æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹çš„ä¸‹ä¸€ä»£äº§å“ï¼Œæ—¨åœ¨æˆä¸ºç»ˆæé€šç”¨æœ¬åœ°æ¨¡å‹ï¼Œæ”¯æŒç¼–ç ã€æ•°å­¦ã€ä»£ç†ã€å‡½æ•°è°ƒç”¨å’Œä¸€èˆ¬ç”¨ä¾‹ã€‚

â€‹                                                              8b

##             zephyr

Zephyr is a series of fine-tuned versions of the Mistral and Mixtral models that are trained to act as helpful assistants.
Zephyr æ˜¯ Mistral å’Œ Mixtral æ¨¡å‹çš„ä¸€ç³»åˆ—å¾®è°ƒç‰ˆæœ¬ï¼Œç»è¿‡è®­ç»ƒï¼Œå¯ä»¥å……å½“æœ‰ç”¨çš„åŠ©æ‰‹ã€‚

â€‹                                                              7b                          141b

##             codestral

Codestral is Mistral AIâ€™s first-ever code model designed for code generation tasks.
Codestral æ˜¯ Mistral AI æœ‰å²ä»¥æ¥ç¬¬ä¸€ä¸ªä¸“ä¸ºä»£ç ç”Ÿæˆä»»åŠ¡è€Œè®¾è®¡çš„ä»£ç æ¨¡å‹ã€‚

â€‹                                                              22b

##             mistral-small

Mistral Small 3 sets a new benchmark in the â€œsmallâ€ Large Language Models category below 70B.
Mistral Small 3 åœ¨ 70B ä»¥ä¸‹çš„â€œå°å‹â€å¤§å‹è¯­è¨€æ¨¡å‹ç±»åˆ«ä¸­æ ‘ç«‹äº†æ–°çš„æ ‡æ†ã€‚

â€‹                                                  tools å·¥å…·                                      22b                          24b

##             olmo2

OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens.  These models are on par with or better than equivalently sized fully  open models, and competitive with open-weight models such as Llama 3.1  on English academic benchmarks.
OLMo 2 æ˜¯ä¸€ä¸ªæ–°çš„ 7B å’Œ 13B æ¨¡å‹ç³»åˆ—ï¼Œå¯åœ¨é«˜è¾¾ 5T çš„ä»¤ç‰Œä¸Šè¿›è¡Œè®­ç»ƒã€‚è¿™äº›æ¨¡å‹ä¸åŒç­‰å¤§å°çš„å®Œå…¨å¼€æ”¾æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½ï¼Œå¹¶ä¸”åœ¨è‹±è¯­å­¦æœ¯åŸºå‡†æµ‹è¯•ä¸­ä¸ Llama 3.1 ç­‰å¼€æ”¾é‡é‡æ¨¡å‹ç«äº‰ã€‚

â€‹                                                              7b                          13b

##             granite-code

A family of open foundation models by IBM for Code Intelligence
IBM é¢å‘ Code Intelligence çš„ä¸€ç³»åˆ—å¼€æ”¾åŸºç¡€æ¨¡å‹

â€‹                                                              3b                          8b                          20b                          34b

##             starcoder          

StarCoder is a code generation model trained on 80+ programming languages.
StarCoder æ˜¯ä¸€ç§åœ¨ 80+ ç¼–ç¨‹è¯­è¨€ä¸Šè®­ç»ƒçš„ä»£ç ç”Ÿæˆæ¨¡å‹ã€‚

â€‹                                                              1b                          3b                          7b                          15b

##             smollm

ğŸª A family of small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset.
ğŸª ä¸€ç³»åˆ—å…·æœ‰ 135Mã€360M å’Œ 1.7B å‚æ•°çš„å°æ¨¡å‹ï¼Œåœ¨æ–°çš„é«˜è´¨é‡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚

â€‹                                                              135m 135 åˆ†é’Ÿ                          360m 360 ç±³                          1.7b

##             wizard-vicuna-uncensored

Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford.
Wizard Vicuna Uncensored æ˜¯ä¸€ä¸ªåŸºäº Eric Hartford çš„ Llama 2 uncensored çš„ 7Bã€13B å’Œ 30B å‚æ•°æ¨¡å‹ã€‚

â€‹                                                              7b                          13b                          30b

##             vicuna

General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.
åŸºäº Llama å’Œ Llama 2 çš„é€šç”¨èŠå¤©æ¨¡å‹ï¼Œä¸Šä¸‹æ–‡å¤§å°ä¸º 2K åˆ° 16Kã€‚

â€‹                                                              7b                          13b                          33b

##             mistral-openorca          

Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset.
Mistral OpenOrca æ˜¯ä¸€ä¸ª 70 äº¿å‚æ•°æ¨¡å‹ï¼Œä½¿ç”¨ OpenOrca æ•°æ®é›†åœ¨ Mistral 7B æ¨¡å‹ä¹‹ä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚

â€‹                                                              7b

##             qwq          

QwQ is an experimental research model focused on advancing AI reasoning capabilities.
QwQ æ˜¯ä¸€ç§å®éªŒç ”ç©¶æ¨¡å‹ï¼Œä¸“æ³¨äºæé«˜ AI æ¨ç†èƒ½åŠ›ã€‚

â€‹                                                  tools å·¥å…·                                      32b

##             llama2-chinese

Llama 2 based model fine tuned to improve Chinese dialogue ability.
åŸºäº Llama 2 çš„æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œä»¥æé«˜ä¸­æ–‡å¯¹è¯èƒ½åŠ›ã€‚

â€‹                                                              7b                          13b

##             openchat

A family of open-source models trained on a wide variety of data,  surpassing ChatGPT on various benchmarks. Updated to version 3.5-0106.
ä¸€ç³»åˆ—åŸºäºå„ç§æ•°æ®è¿›è¡Œè®­ç»ƒçš„å¼€æºæ¨¡å‹ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­éƒ½è¶…è¿‡äº† ChatGPTã€‚æ›´æ–°è‡³ç‰ˆæœ¬ 3.5-0106ã€‚

â€‹                                                              7b

##             codegeex4

A versatile model for AI software development scenarios, including code completion.
é€‚ç”¨äº AI è½¯ä»¶å¼€å‘åœºæ™¯çš„é€šç”¨æ¨¡å‹ï¼ŒåŒ…æ‹¬ä»£ç å®Œæˆã€‚

â€‹                                                              9b

##             aya

Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages. 
Aya 23 ç”± Cohere å‘å¸ƒï¼Œæ˜¯æ”¯æŒ 23 ç§è¯­è¨€çš„æœ€å…ˆè¿›çš„å¤šè¯­è¨€æ¨¡å‹çš„æ–°ç³»åˆ—ã€‚

â€‹                                                              8b                          35b

##             codeqwen          

CodeQwen1.5 is a large language model pretrained on a large amount of code data.
CodeQwen1.5 æ˜¯ä¸€ä¸ªåŸºäºå¤§é‡ä»£ç æ•°æ®è¿›è¡Œé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚

â€‹                                                              7b

##             deepseek-llm

An advanced language model crafted with 2 trillion bilingual tokens.
ä½¿ç”¨ 2 ä¸‡äº¿ä¸ªåŒè¯­æ ‡è®°åˆ¶ä½œçš„é«˜çº§è¯­è¨€æ¨¡å‹ã€‚

7b                          67b

##             mistral-large          

Mistral Large 2 is Mistral's new flagship model that is significantly more  capable in code generation, mathematics, and reasoning with 128k context window and support for dozens of languages.
Mistral Large 2 æ˜¯ Mistral çš„æ–°æ——èˆ°æ¨¡å‹ï¼Œå®ƒåœ¨ä»£ç ç”Ÿæˆã€æ•°å­¦å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›è¦å¼ºå¾—å¤šï¼Œå…·æœ‰ 128k ä¸Šä¸‹æ–‡çª—å£ï¼Œå¹¶æ”¯æŒæ•°åç§è¯­è¨€ã€‚

tools å·¥å…·                                      123b

##             deepseek-v2

A strong, economical, and efficient Mixture-of-Experts language model.
ä¸€ä¸ªå¼ºå¤§ã€ç»æµä¸”é«˜æ•ˆçš„ Mixture-of-Experts è¯­è¨€æ¨¡å‹ã€‚

16b                          236b

##             nous-hermes2

The powerful family of models by Nous Research that excels at scientific discussion and coding tasks.
Nous Research å¼ºå¤§çš„æ¨¡å‹ç³»åˆ—ï¼Œæ“…é•¿ç§‘å­¦è®¨è®ºå’Œç¼–ç ä»»åŠ¡ã€‚

10.7b                          34b

##             glm4          

A strong multi-lingual general language model with competitive performance to Llama 3.
å¼ºå¤§çš„å¤šè¯­è¨€é€šç”¨è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰ä¸ Llama 3 ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚

9b

##             stable-code ç¨³å®šä»£ç           

Stable Code 3B is a coding model with instruct and code completion variants on par with models such as Code Llama 7B that are 2.5x larger.
Stable Code 3B æ˜¯ä¸€ç§ç¼–ç æ¨¡å‹ï¼Œå…·æœ‰ instruct å’Œ code completion å˜ä½“ï¼Œä¸ Code Llama 7B ç­‰æ¨¡å‹ç›¸åŒï¼Œåè€…çš„å°ºå¯¸æ˜¯ 2.5 å€ã€‚

3b

##             openhermes          

OpenHermes 2.5 is a 7B model fine-tuned by Teknium on Mistral with fully open datasets.
OpenHermes 2.5 æ˜¯ç”± Teknium åœ¨ Mistral ä¸Šå¾®è°ƒçš„ 7B æ¨¡å‹ï¼Œå…·æœ‰å®Œå…¨å¼€æ”¾çš„æ•°æ®é›†ã€‚

##             qwen2-math

Qwen2 Math is a series of specialized math language models built upon the  Qwen2 LLMs, which significantly outperforms the mathematical  capabilities of open-source models and even closed-source models (e.g.,  GPT4o).
Qwen2 Math æ˜¯ä¸€ç³»åˆ—åŸºäº Qwen2 æ„å»ºçš„ä¸“ç”¨æ•°å­¦è¯­è¨€æ¨¡å‹LLMsï¼Œå…¶æ•°å­¦èƒ½åŠ›æ˜æ˜¾ä¼˜äºå¼€æºæ¨¡å‹ç”šè‡³é—­æºæ¨¡å‹ï¼ˆä¾‹å¦‚ GPT4oï¼‰ã€‚

1.5b                          7b                          72b

##             command-r-plus

Command R+ is a powerful, scalable large language model purpose-built to excel at real-world enterprise use cases.
Command R+ æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§ã€å¯æ‰©å±•çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¸“ä¸ºåœ¨å®é™…ä¼ä¸šç”¨ä¾‹ä¸­è„±é¢–è€Œå‡ºè€Œæ„å»ºã€‚

tools å·¥å…·                                      104b

##             tinydolphin

An experimental 1.1B parameter model trained on the new Dolphin 2.8 dataset by Eric Hartford and based on TinyLlama.
ç”± Eric Hartford åœ¨æ–°çš„ Dolphin 2.8 æ•°æ®é›†ä¸Šè®­ç»ƒçš„å®éªŒæ€§ 1.1B å‚æ•°æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäº TinyLlamaã€‚

1.1b

##             wizardcoder

State-of-the-art code generation model
æœ€å…ˆè¿›çš„ä»£ç ç”Ÿæˆæ¨¡å‹

33b 

##             moondream

moondream2 is a small vision language model designed to run efficiently on edge devices.
MoonDream2 æ˜¯ä¸€ç§å°å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šé«˜æ•ˆè¿è¡Œã€‚

vision è§†è§‰                                                              1.8b

##             bakllava

BakLLaVA is a multimodal model consisting of the Mistral 7B base model augmented with the LLaVA  architecture.
BakLLaVA æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¨¡å‹ï¼Œç”± Mistral 7B åŸºæœ¬æ¨¡å‹ç»„æˆï¼Œå¹¶å¢å¼ºäº† LLaVA æ¶æ„ã€‚

vision è§†è§‰                                                              7b

##             stablelm2

Stable LM 2 is a state-of-the-art 1.6B and 12B parameter language model  trained on multilingual data in English, Spanish, German, Italian,  French, Portuguese, and Dutch.
Stable LM 2 æ˜¯æœ€å…ˆè¿›çš„ 1.6B å’Œ 12B å‚æ•°è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºè‹±è¯­ã€è¥¿ç­ç‰™è¯­ã€å¾·è¯­ã€æ„å¤§åˆ©è¯­ã€æ³•è¯­ã€è‘¡è„ç‰™è¯­å’Œè·å…°è¯­çš„å¤šè¯­è¨€æ•°æ®è¿›è¡Œè®­ç»ƒã€‚

1.6b                          12b

##             neural-chat

A fine-tuned model based on Mistral with good coverage of domain and language.
åŸºäº Mistral çš„å¾®è°ƒæ¨¡å‹ï¼Œå…·æœ‰è‰¯å¥½çš„åŸŸå’Œè¯­è¨€è¦†ç›–ç‡ã€‚

7b

##             reflection

A high-performing model trained with a new technique called  Reflection-tuning that teaches a LLM to detect mistakes in its reasoning and correct course.
ä¸€ç§é«˜æ€§èƒ½æ¨¡å‹ï¼Œä½¿ç”¨ä¸€ç§ç§°ä¸º Reflection-tuning çš„æ–°æŠ€æœ¯è¿›è¡Œè®­ç»ƒï¼Œè¯¥æŠ€æœ¯æ•™ a LLM æ£€æµ‹å…¶æ¨ç†ä¸­çš„é”™è¯¯å¹¶çº æ­£è¿‡ç¨‹ã€‚

70b

##             wizard-math

Model focused on math and logic problems
æ¨¡å‹ä¾§é‡äºæ•°å­¦å’Œé€»è¾‘é—®é¢˜

7b                          13b                          70b

##             llama3-gradient

This model extends LLama-3 8B's context length from 8k to over 1m tokens.
æ­¤æ¨¡å‹å°† LLama-3 8B çš„ä¸Šä¸‹æ–‡é•¿åº¦ä» 8k æ‰©å±•åˆ°è¶…è¿‡ 1m ä»¤ç‰Œã€‚

8b                          70b

##             llama3-chatqa          

A model from NVIDIA based on Llama 3 that excels at conversational  question answering (QA) and retrieval-augmented generation (RAG).
NVIDIA åŸºäº Llama 3 çš„æ¨¡å‹ï¼Œæ“…é•¿å¯¹è¯é—®ç­” ï¼ˆQAï¼‰ å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆ ï¼ˆRAGï¼‰ã€‚

8b                          70b

##             sqlcoder

SQLCoder is a code completion model fined-tuned on StarCoder for SQL generation tasks
SQLCoder æ˜¯ä¸€ç§ä»£ç å®Œæˆæ¨¡å‹ï¼Œåœ¨ StarCoder ä¸Šé’ˆå¯¹ SQL ç”Ÿæˆä»»åŠ¡è¿›è¡Œäº†å¾®è°ƒ

7b                          15b

##             bge-large          

Embedding model from BAAI mapping texts to vectors.
å°† BAAI ä¸­çš„æ¨¡å‹åµŒå…¥åˆ°å‘é‡ã€‚

embedding åµŒå…¥                                                  335m

##             xwinlm          

Conversational model based on Llama 2 that performs competitively on various benchmarks.
åŸºäº Llama 2 çš„å¯¹è¯æ¨¡å‹ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å…·æœ‰ç«äº‰åŠ›ã€‚

7b                          13b

##             dolphincoder          

A 7B and 15B uncensored variant of the Dolphin model family that excels at coding, based on StarCoder2.
Dolphin æ¨¡å‹ç³»åˆ—çš„ 7B å’Œ 15Bå˜ä½“ï¼Œæ“…é•¿ç¼–ç ï¼ŒåŸºäº StarCoder2ã€‚

7b                          15b

##             nous-hermes

General use models based on Llama and Llama 2 from Nous Research.
åŸºäº Nous Research çš„ Llama å’Œ Llama 2 çš„é€šç”¨æ¨¡å‹ã€‚

7b                          13b

##             phind-codellama          

Code generation model based on Code Llama.
åŸºäº Code Llama çš„ä»£ç ç”Ÿæˆæ¨¡å‹ã€‚

34b       

##             llava-phi3

A new small LLaVA model fine-tuned from Phi 3 Mini.
ä» Phi 3 Mini å¾®è°ƒè€Œæ¥çš„æ–°å‹å°å‹ LLaVA å‹å·ã€‚

â€‹                        vision è§†è§‰                                                              3.8b

##             yarn-llama2

An extension of Llama 2 that supports a context of up to 128k tokens.
Llama 2 çš„æ‰©å±•ï¼Œæ”¯æŒæœ€å¤š 128k ä¸ªä»¤ç‰Œçš„ä¸Šä¸‹æ–‡ã€‚

7b                          13b

##             solar

A compact, yet powerful 10.7B large language model designed for single-turn conversation.
ä¸€ä¸ªç´§å‡‘ä½†åŠŸèƒ½å¼ºå¤§çš„ 10.7B å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¸“ä¸ºå•è½®å¯¹è¯è€Œè®¾è®¡ã€‚

10.7b

##             granite3.1-dense

The IBM Granite 2B and 8B models are text-only dense LLMs trained on over  12 trillion tokens of data, demonstrated significant improvements over  their predecessors in performance and speed in IBMâ€™s initial testing.
IBM Granite 2B å’Œ 8B æ¨¡å‹æ˜¯çº¯æ–‡æœ¬å¯†é›†LLMsæ¨¡å‹ï¼Œåœ¨è¶…è¿‡ 12 ä¸‡äº¿ä¸ªæ•°æ®ä»¤ç‰Œä¸Šè¿›è¡Œäº†å¯†é›†è®­ç»ƒï¼Œåœ¨ IBM çš„åˆå§‹æµ‹è¯•ä¸­ï¼Œå…¶æ€§èƒ½å’Œé€Ÿåº¦æ¯”å…¶å‰èº«æœ‰äº†æ˜¾è‘—æ”¹è¿›ã€‚

tools	2b                          8b

##             starling-lm          

Starling is a large language model trained by reinforcement learning from AI feedback focused on improving chatbot helpfulness.
Starling æ˜¯ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¯¹ AI åé¦ˆè¿›è¡Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä¸“æ³¨äºæé«˜èŠå¤©æœºå™¨äººçš„å®ç”¨æ€§ã€‚

â€‹                                                              7b

##             athene-v2

Athene-V2 is a 72B parameter model which excels at code completion, mathematics, and log extraction tasks.
Athene-V2 æ˜¯ä¸€ä¸ª 72B å‚æ•°æ¨¡å‹ï¼Œæ“…é•¿ä»£ç å®Œæˆã€æ•°å­¦å’Œå¯¹æ•°æå–ä»»åŠ¡ã€‚

â€‹                                                  tools å·¥å…·                                      72b

##             wizardlm

General use model based on Llama 2.
åŸºäº Llama 2 çš„é€šç”¨æ¨¡å‹ã€‚

##             yi-coder          

Yi-Coder is a series of open-source code language models that delivers  state-of-the-art coding performance with fewer than 10 billion  parameters.
Yi-Coder æ˜¯ä¸€ç³»åˆ—å¼€æºä»£ç è¯­è¨€æ¨¡å‹ï¼Œå¯æä¾›æœ€å…ˆè¿›çš„ç¼–ç æ€§èƒ½ï¼Œå‚æ•°å°‘äº 100 äº¿ä¸ªã€‚

1.5b                          9b

##             internlm2          

InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding reasoning capability.
InternLM2.5 æ˜¯ä¸€ä¸ªä¸ºå®é™…åœºæ™¯é‡èº«å®šåˆ¶çš„ 7B å‚æ•°æ¨¡å‹ï¼Œå…·æœ‰å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€‚

1m	1.8b	7b	20b

##             samantha-mistral

A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistral.
æ¥å—è¿‡å“²å­¦ã€å¿ƒç†å­¦å’Œäººé™…å…³ç³»åŸ¹è®­çš„åŒä¼´åŠ©ç†ã€‚åŸºäº Mistralã€‚

â€‹                                                              7b

##             falcon

A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and chat bots.
ç”±æŠ€æœ¯åˆ›æ–°ç ”ç©¶æ‰€ ï¼ˆTIIï¼‰ æ„å»ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”¨äºæ‘˜è¦ã€æ–‡æœ¬ç”Ÿæˆå’ŒèŠå¤©æœºå™¨äººã€‚

â€‹                                                              7b                          40b 40 å­—èŠ‚                          180b

##             nemotron-mini

A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling.
NVIDIA çš„å•†ä¸šå‹å¥½å‹å°è¯­è¨€æ¨¡å‹ï¼Œé’ˆå¯¹è§’è‰²æ‰®æ¼”ã€RAG QA å’Œå‡½æ•°è°ƒç”¨è¿›è¡Œäº†ä¼˜åŒ–ã€‚

â€‹                                                  tools å·¥å…·                                      4b

##             nemotron

Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the  helpfulness of LLM generated responses to user queries.
Llama-3.1-Nemotron-70B-Instruct æ˜¯ç”± NVIDIA å®šåˆ¶çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”¨äºæé«˜ç”Ÿæˆçš„å“åº”å¯¹ç”¨æˆ·æŸ¥è¯¢çš„LLMå¸®åŠ©æ€§ã€‚

â€‹                                                  tools å·¥å…·                                      70b

##             dolphin-phi

2.7B uncensored Dolphin model by Eric Hartford, based on the Phi language model by Microsoft Research.
2.7B ç”± Eric Hartford æä¾›çš„æœªç»å®¡æŸ¥çš„ Dolphin æ¨¡å‹ï¼ŒåŸºäº Microsoft Research çš„ Phi è¯­è¨€æ¨¡å‹ã€‚

â€‹                                                              2.7b

##             orca2

Orca 2 is built by Microsoft research, and are a fine-tuned version of  Meta's Llama 2 models.  The model is designed to excel particularly in  reasoning.
Orca 2 ç”± Microsoft Research æ„å»ºï¼Œæ˜¯ Meta çš„ Llama 2 æ¨¡å‹çš„å¾®è°ƒç‰ˆæœ¬ã€‚è¯¥æ¨¡å‹æ—¨åœ¨ç‰¹åˆ«æ“…é•¿æ¨ç†ã€‚

â€‹                                                              7b                          13b

##             deepscaler

A fine-tuned version of Deepseek-R1-Distilled-Qwen-1.5B that surpasses  the performance of OpenAIâ€™s o1-preview with just 1.5B parameters on  popular math evaluations.
Deepseek-R1-Distilled-Qwen-1.5B çš„å¾®è°ƒç‰ˆæœ¬ï¼Œåœ¨æµè¡Œçš„æ•°å­¦è¯„ä¼°ä¸­ä»…å…·æœ‰ 1.5B å‚æ•°ï¼Œå…¶æ€§èƒ½è¶…è¿‡äº† OpenAI çš„ o1-previewã€‚

â€‹                                                              1.5b

##             wizardlm-uncensored          

Uncensored version of Wizard LM model 
Wizard LM æ¨¡å‹çš„æœªç»å®¡æŸ¥ç‰ˆæœ¬

â€‹                                                              13b

##             stable-beluga

Llama 2 based model fine tuned on an Orca-style dataset. Originally called Free Willy.
åŸºäº Llama 2 çš„æ¨¡å‹åœ¨ Orca é£æ ¼çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚åŸå Free Willyã€‚

â€‹                                                              7b                          13b                          70b

##             granite3-dense

The IBM Granite 2B and 8B models are designed to support tool-based use  cases and support for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.
IBM Granite 2B å’Œ 8B æ¨¡å‹æ—¨åœ¨æ”¯æŒåŸºäºå·¥å…·çš„ç”¨ä¾‹ï¼Œå¹¶æ”¯æŒæ£€ç´¢å¢å¼ºç”Ÿæˆ ï¼ˆRAGï¼‰ï¼Œä»è€Œç®€åŒ–ä»£ç ç”Ÿæˆã€è½¬æ¢å’Œé”™è¯¯ä¿®å¤ã€‚

â€‹                                                  tools å·¥å…·                                      2b                          8b

##             llama3-groq-tool-use

A series of models from Groq that represent a significant advancement in  open-source AI capabilities for tool use/function calling.
æ¥è‡ª Groq çš„ä¸€ç³»åˆ—æ¨¡å‹ï¼Œä»£è¡¨äº†ç”¨äºå·¥å…·ä½¿ç”¨/å‡½æ•°è°ƒç”¨çš„å¼€æº AI åŠŸèƒ½çš„é‡å¤§è¿›æ­¥ã€‚

â€‹                                                  tools å·¥å…·                                      8b                          70b

##             deepseek-v2.5

An upgraded version of DeekSeek-V2  that integrates the general and coding abilities of both DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct.
DeekSeek-V2 çš„å‡çº§ç‰ˆæœ¬ï¼Œé›†æˆäº† DeepSeek-V2-Chat å’Œ DeepSeek-Coder-V2-Struct çš„é€šç”¨å’Œç¼–ç èƒ½åŠ›ã€‚

236b       

##             medllama2

Fine-tuned Llama 2 model to answer medical questions based on an open source medical dataset. 
å¾®è°ƒ Llama 2 æ¨¡å‹ï¼Œä»¥åŸºäºå¼€æºåŒ»å­¦æ•°æ®é›†å›ç­”åŒ»å­¦é—®é¢˜ã€‚

7b

##             meditron

Open-source medical large language model adapted from Llama 2 to the medical domain.
ä» Llama 2 æ”¹ç¼–åˆ°åŒ»å­¦é¢†åŸŸçš„å¼€æºåŒ»å­¦å¤§è¯­è¨€æ¨¡å‹ã€‚

7b	70b

##             llama-pro

An expansion of Llama 2 that specializes in integrating both general  language understanding and domain-specific knowledge, particularly in  programming and mathematics.
Llama 2 çš„æ‰©å±•ï¼Œä¸“é—¨ç”¨äºæ•´åˆä¸€èˆ¬è¯­è¨€ç†è§£å’Œç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼–ç¨‹å’Œæ•°å­¦æ–¹é¢ã€‚

##             smallthinker

A new small reasoning model fine-tuned from the Qwen 2.5 3B Instruct model.
ä» Qwen 2.5 3B Instruct æ¨¡å‹å¾®è°ƒè€Œæ¥çš„å…¨æ–°å°æ¨ç†æ¨¡å‹ã€‚

3b

##             yarn-mistral

An extension of Mistral to support context windows of 64K or 128K.
Mistral çš„æ‰©å±•ï¼Œå¯æ”¯æŒ 64K æˆ– 128K çš„ä¸Šä¸‹æ–‡çª—å£ã€‚

7b

##             aya-expanse

Cohere For AI's language models trained to perform well across 23 different languages.
Cohere For AI çš„è¯­è¨€æ¨¡å‹ç»è¿‡è®­ç»ƒï¼Œå¯åœ¨ 23 ç§ä¸åŒçš„è¯­è¨€ä¸­è¡¨ç°è‰¯å¥½ã€‚

tools å·¥å…·	8b	32b

##             paraphrase-multilingual

Sentence-transformers model that can be used for tasks like clustering or semantic search.
å¯ç”¨äºèšç±»åˆ†ææˆ–è¯­ä¹‰æœç´¢ç­‰ä»»åŠ¡çš„å¥å­è½¬æ¢å™¨æ¨¡å‹ã€‚

embedding

##             granite3-moe

The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage.
IBM Granite 1B å’Œ 3B æ¨¡å‹æ˜¯ IBM çš„é¦–ä¸ªä¸“å®¶ ï¼ˆMoEï¼‰ Granite æ¨¡å‹æ··åˆï¼Œä¸“ä¸ºä½å»¶è¿Ÿä½¿ç”¨è€Œè®¾è®¡ã€‚

tools	1b	3b

##             nexusraven          

Nexus Raven is a 13B instruction tuned model for function calling tasks. 
Nexus Raven æ˜¯ä¸€ä¸ªç”¨äºå‡½æ•°è°ƒç”¨ä»»åŠ¡çš„ 13B æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ã€‚

13b

##             codeup

Great code generation model based on Llama2.
åŸºäº Llama2 çš„å‡ºè‰²ä»£ç ç”Ÿæˆæ¨¡å‹ã€‚

13b            

##             falcon3

A family of efficient AI models under 10B parameters performant in  science, math, and coding through innovative training techniques.
ä¸€ç³»åˆ—ä½äº 10B å‚æ•°çš„é«˜æ•ˆ AI æ¨¡å‹ï¼Œé€šè¿‡åˆ›æ–°çš„è®­ç»ƒæŠ€æœ¯åœ¨ç§‘å­¦ã€æ•°å­¦å’Œç¼–ç æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚

1b	3b	7b	10b

##             nous-hermes2-mixtral          

The Nous Hermes 2 model from Nous Research, now trained over Mixtral.
æ¥è‡ª Nous Research çš„ Nous Hermes 2 æ¨¡å‹ï¼Œç°åœ¨åœ¨ Mixtral ä¸Šè®­ç»ƒã€‚

8x7b

##             everythinglm

Uncensored Llama2 based model with support for a 16K context window.
åŸºäº Llama2 çš„æœªç»å®¡æŸ¥çš„æ¨¡å‹ï¼Œæ”¯æŒ 16K ä¸Šä¸‹æ–‡çª—å£ã€‚

13b

##             shieldgemma

ShieldGemma is set of instruction tuned models for evaluating the safety of text  prompt input and text output responses against a set of defined safety  policies.
ShieldGemma æ˜¯ä¸€ç»„æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ï¼Œç”¨äºæ ¹æ®ä¸€ç»„å®šä¹‰çš„å®‰å…¨ç­–ç•¥è¯„ä¼°æ–‡æœ¬æç¤ºè¾“å…¥å’Œæ–‡æœ¬è¾“å‡ºå“åº”çš„å®‰å…¨æ€§ã€‚

2b	9b	27b

##             granite3.1-moe

The IBM Granite 1B and 3B models are long-context mixture of experts (MoE)  Granite models from IBM designed for low latency usage.
IBM Granite 1B å’Œ 3B æ¨¡å‹æ˜¯ IBM ä¸“å®¶ ï¼ˆMoEï¼‰ Granite æ¨¡å‹çš„é•¿æœŸä¸Šä¸‹æ–‡æ··åˆï¼Œä¸“ä¸ºä½å»¶è¿Ÿä½¿ç”¨è€Œè®¾è®¡ã€‚

tools	1b	3b

##             snowflake-arctic-embed2          

Snowflake's frontier embedding model. Arctic Embed 2.0 adds multilingual support  without sacrificing English performance or scalability.
Snowflake çš„ frontier åµŒå…¥æ¨¡å‹ã€‚Arctic Embed 2.0 å¢åŠ äº†å¤šè¯­è¨€æ”¯æŒï¼Œè€Œä¸ä¼šç‰ºç‰²è‹±è¯­æ€§èƒ½æˆ–å¯æ‰©å±•æ€§ã€‚

embedding

##             falcon2

Falcon2 is an 11B parameters causal decoder-only model built by TII and trained over 5T tokens.
Falcon2 æ˜¯ç”± TII æ„å»ºçš„ 11B å‚æ•°å› æœè§£ç å™¨ä¸“ç”¨æ¨¡å‹ï¼Œå¹¶é€šè¿‡ 5T ä»¤ç‰Œè¿›è¡Œè®­ç»ƒã€‚

11b

##             magicoder

ğŸ© Magicoder is a family of 7B parameter models trained on 75K synthetic  instruction data using OSS-Instruct, a novel approach to enlightening  LLMs with open-source code snippets.
ğŸ© Magicoder æ˜¯ä¸€ç³»åˆ— 7B å‚æ•°æ¨¡å‹ï¼Œä½¿ç”¨ OSS-Struct åœ¨ 75K åˆæˆæŒ‡ä»¤æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒOSS-Struct æ˜¯ä¸€ç§LLMsä½¿ç”¨å¼€æºä»£ç ç‰‡æ®µè¿›è¡Œå¯å‘çš„æ–°æ–¹æ³•ã€‚

7b

##             mathstral

MathÎ£tral: a 7B model designed for math reasoning and scientific discovery by Mistral AI.
MathÎ£tralï¼šç”± Mistral AI ä¸ºæ•°å­¦æ¨ç†å’Œç§‘å­¦å‘ç°è€Œè®¾è®¡çš„ 7B æ¨¡å‹ã€‚

7b                

##             marco-o1

An open large reasoning model for real-world solutions by the Alibaba International Digital Commerce Group (AIDC-AI).
é˜¿é‡Œå·´å·´å›½é™…æ•°å­—å•†åŠ¡é›†å›¢ ï¼ˆAIDC-AIï¼‰ ä¸ºå®é™…è§£å†³æ–¹æ¡ˆæä¾›çš„å¼€æ”¾å¼å¤§å‹æ¨ç†æ¨¡å‹ã€‚

7b

##             stablelm-zephyr          

A lightweight chat model allowing accurate, and responsive output without requiring high-end hardware.
è½»é‡çº§èŠå¤©æ¨¡å‹ï¼Œæ— éœ€é«˜ç«¯ç¡¬ä»¶å³å¯å®ç°å‡†ç¡®ã€å“åº”è¿…é€Ÿçš„è¾“å‡ºã€‚

3b

##             reader-lm

A series of models that convert HTML content to Markdown content, which is useful for content conversion tasks.
å°† HTML å†…å®¹è½¬æ¢ä¸º Markdown å†…å®¹çš„ä¸€ç³»åˆ—æ¨¡å‹ï¼Œè¿™å¯¹äºå†…å®¹è½¬æ¢ä»»åŠ¡éå¸¸æœ‰ç”¨ã€‚

0.5b	1.5b

##             solar-pro

Solar Pro Preview: an advanced large language model (LLM) with 22 billion parameters designed to fit into a single GPU
Solar Pro Previewï¼šå…·æœ‰ 220 äº¿ä¸ªå‚æ•°çš„é«˜çº§å¤§å‹è¯­è¨€æ¨¡å‹ ï¼ˆLLMï¼‰ï¼Œæ—¨åœ¨é€‚åº”å•ä¸ª GPU

22b

##             codebooga

A high-performing code instruct model created by merging two existing code models.
é€šè¿‡åˆå¹¶ä¸¤ä¸ªç°æœ‰ä»£ç æ¨¡å‹åˆ›å»ºçš„é«˜æ€§èƒ½ä»£ç æŒ‡ä»¤æ¨¡å‹ã€‚

34b             

##             duckdb-nsql          

7B parameter text-to-SQL model made by MotherDuck and Numbers Station.
ç”± MotherDuck å’Œ Numbers Station åˆ¶ä½œçš„ 7B å‚æ•°æ–‡æœ¬åˆ° SQL æ¨¡å‹ã€‚

7b            

##             mistrallite

MistralLite is a fine-tuned model based on Mistral with enhanced capabilities of processing long contexts.
MistralLite æ˜¯åŸºäº Mistral çš„å¾®è°ƒæ¨¡å‹ï¼Œå…·æœ‰å¤„ç†é•¿ä¸Šä¸‹æ–‡çš„å¢å¼ºèƒ½åŠ›ã€‚

7b            

##             wizard-vicuna

Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj.
Wizard Vicuna æ˜¯ä¸€ä¸ªåŸºäº Llama 2 çš„ 13B å‚æ•°æ¨¡å‹ï¼Œç”± MelodysDreamj è®­ç»ƒã€‚

13b

##             llama-guard3

Llama Guard 3 is a series of models fine-tuned for content safety classification of LLM inputs and responses.
Llama Guard 3 æ˜¯ä¸€ç³»åˆ—é’ˆå¯¹LLMè¾“å…¥å’Œå“åº”çš„å†…å®¹å®‰å…¨åˆ†ç±»è¿›è¡Œå¾®è°ƒçš„æ¨¡å‹ã€‚

1b	8b                       

##             exaone3.5

EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32B parameters, developed and  released by LG AI Research. 
EXAONE 3.5 æ˜¯ç”± LG AI Research å¼€å‘å’Œå‘å¸ƒçš„ä¸€ç³»åˆ—æ•™å­¦è°ƒæ•´çš„åŒè¯­ï¼ˆè‹±è¯­å’ŒéŸ©è¯­ï¼‰ç”Ÿæˆæ¨¡å‹ï¼ŒèŒƒå›´ä» 2.4B åˆ° 32B å‚æ•°ã€‚

2.4b	7.8b	32b

##             megadolphin

MegaDolphin-2.2-120b is a transformation of Dolphin-2.2-70b created by interleaving the model with itself.
MegaDolphin-2.2-120b æ˜¯ Dolphin-2.2-70b çš„è½¬æ¢ï¼Œé€šè¿‡æ¨¡å‹ä¸è‡ªèº«äº¤é”™åˆ›å»ºã€‚

120b

##             nuextract          

A 3.8B model fine-tuned on a private high-quality synthetic dataset for information extraction, based on Phi-3.
ä¸€ä¸ª 3.8B æ¨¡å‹ï¼ŒåŸºäº Phi-3 åœ¨ç§æœ‰é«˜è´¨é‡åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œç”¨äºä¿¡æ¯æå–ã€‚

3.8b

##             opencoder

OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B  models, supporting chat in English and Chinese languages.
OpenCoder æ˜¯ä¸€ä¸ªå¼€æ”¾ä¸”å¯å¤ç°çš„ä»£ç LLMç³»åˆ—ï¼ŒåŒ…æ‹¬ 1.5B å’Œ 8B æ¨¡å‹ï¼Œæ”¯æŒè‹±æ–‡å’Œä¸­æ–‡èŠå¤©ã€‚

1.5b	8b

##             notux          

A top-performing mixture of experts model, fine-tuned with high-quality data.
æ€§èƒ½æœ€ä½³çš„ä¸“å®¶æ¨¡å‹ç»„åˆï¼Œä½¿ç”¨é«˜è´¨é‡æ•°æ®è¿›è¡Œå¾®è°ƒã€‚

8x7b

##             open-orca-platypus2

Merge of the Open Orca OpenChat model and the Garage-bAInd Platypus 2 model. Designed for chat and code generation.
Open Orca OpenChat æ¨¡å‹å’Œ Garage-bAInd Platypus 2 æ¨¡å‹çš„åˆå¹¶ã€‚ä¸“ä¸ºèŠå¤©å’Œä»£ç ç”Ÿæˆè€Œè®¾è®¡ã€‚

13b       

##             notus

A 7B chat model fine-tuned with high-quality data and based on Zephyr.
ä¸€ä¸ªåŸºäº Zephyr çš„ 7B èŠå¤©æ¨¡å‹ï¼Œä½¿ç”¨é«˜è´¨é‡æ•°æ®è¿›è¡Œå¾®è°ƒã€‚

7b

##             goliath

A language model created by combining two fine-tuned Llama 2 70B models into one.
é€šè¿‡å°†ä¸¤ä¸ªå¾®è°ƒçš„ Llama 2 70B æ¨¡å‹åˆå¹¶ä¸ºä¸€ä¸ªæ¨¡å‹è€Œåˆ›å»ºçš„è¯­è¨€æ¨¡å‹ã€‚ 

##             bespoke-minicheck

A state-of-the-art fact-checking model developed by Bespoke Labs.
ç”± Bespoke Labs å¼€å‘çš„æœ€å…ˆè¿›çš„äº‹å®æ ¸æŸ¥æ¨¡å‹ã€‚

7b

##             command-r7b     

The smallest model in Cohere's R series delivers top-tier speed,  efficiency, and quality to build powerful AI applications on commodity  GPUs and edge devices.
Cohere çš„ R ç³»åˆ—ä¸­æœ€å°çš„å‹å·å¯æä¾›ä¸€æµçš„é€Ÿåº¦ã€æ•ˆç‡å’Œè´¨é‡ï¼Œä»¥åœ¨å•†ç”¨ GPU å’Œè¾¹ç¼˜è®¾å¤‡ä¸Šæ„å»ºå¼ºå¤§çš„ AI åº”ç”¨ç¨‹åºã€‚

tools	7b

##             firefunction-v2

An open weights function calling model based on Llama 3, competitive with GPT-4o function calling capabilities.
ä¸€ç§åŸºäº Llama 3 çš„å¼€æ”¾æƒé‡å‡½æ•°è°ƒç”¨æ¨¡å‹ï¼Œä¸ GPT-4o å‡½æ•°è°ƒç”¨èƒ½åŠ›ç›¸åª²ç¾ã€‚

tools	70b

##             tulu3

TÃ¼lu 3 is a leading instruction following model family, offering fully  open-source data, code, and recipes by the The Allen Institute for AI.
TÃ¼lu 3 æ˜¯éµå¾ªæ¨¡å‹ç³»åˆ—çš„é¢†å…ˆæŒ‡ä»¤ï¼Œæä¾›ç”± Allen Institute for AI æä¾›çš„å®Œå…¨å¼€æºæ•°æ®ã€ä»£ç å’Œé…æ–¹ã€‚

8b	70b

##             dbrx          

DBRX is an open, general-purpose LLM created by Databricks.
DBRX æ˜¯ç”± Databricks LLM åˆ›å»ºçš„å¼€æ”¾å¼é€šç”¨å·¥å…·ã€‚

132b

##             granite-embedding

The IBM Granite Embedding 30M and 278M models models are text-only dense  biencoder embedding models, with 30M available in English only and 278M  serving multilingual use cases.
IBM Granite Embedding 30M å’Œ 278M æ¨¡å‹æ¨¡å‹æ˜¯çº¯æ–‡æœ¬å¯†é›†åŒç¼–ç å™¨åµŒå…¥æ¨¡å‹ï¼Œå…¶ä¸­ 30M ä»…æä¾›è‹±æ–‡ç‰ˆæœ¬ï¼Œ278M æœåŠ¡äºå¤šè¯­è¨€ç”¨ä¾‹ã€‚

embedding          

##             granite3-guardian

The IBM Granite Guardian 3.0 2B and 8B models are designed to detect risks in prompts and/or responses.
IBM Granite Guardian 3.0 2B å’Œ 8B æ¨¡å‹æ—¨åœ¨æ£€æµ‹æç¤ºå’Œ/æˆ–å“åº”ä¸­çš„é£é™©ã€‚

2b	8b

##             alfred

A robust conversational model designed to be used for both chat and instruct use cases.
ä¸€ä¸ªå¼ºå¤§çš„å¯¹è¯æ¨¡å‹ï¼Œæ—¨åœ¨ç”¨äºèŠå¤©å’ŒæŒ‡å¯¼ä½¿ç”¨æ¡ˆä¾‹ã€‚

40b

##             sailor2

Sailor2 æ˜¯ä¸“ä¸ºä¸œå—äºšæ‰“é€ çš„å¤šè¯­è¨€è¯­è¨€æ¨¡å‹ã€‚æä¾› 1Bã€8B å’Œ 20B å‚æ•°å¤§å°ã€‚

1b	8b	20b

##             r1-1776

A version of the DeepSeek-R1 model that has been post trained to provide  unbiased, accurate, and factual information by Perplexity. 
Perplexity å·²ç»è¿‡åè®­ç»ƒçš„ DeepSeek-R1 æ¨¡å‹ç‰ˆæœ¬ï¼Œå¯æä¾›å…¬æ­£ã€å‡†ç¡®å’ŒçœŸå®çš„ä¿¡æ¯ã€‚

70b	671b

##             granite3.2

Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for thinking capabilities.
Granite-3.2 æ˜¯ IBM Granite çš„ä¸€ç³»åˆ—é•¿ä¸Šä¸‹æ–‡ AI æ¨¡å‹ï¼Œé’ˆå¯¹æ€ç»´èƒ½åŠ›è¿›è¡Œäº†å¾®è°ƒã€‚

tools	2b	8b
